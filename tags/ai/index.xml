<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI - 标签 - XBruce Blog</title>
    <link>https://ansha886.github.io/tags/ai/</link>
    <description>@xbruce&#39;s works</description>
    <generator>Hugo 0.134.3 &amp; FixIt v0.3.15</generator>
    <language>en</language>
    <managingEditor>licheng0601@gmail.com (Bruce)</managingEditor>
    <webMaster>licheng0601@gmail.com (Bruce)</webMaster>
    <lastBuildDate>Sat, 14 Dec 2024 11:40:31 +0800</lastBuildDate>
    <atom:link href="https://ansha886.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>商学合作，浙大联合快手等的多相机视频生成系统：SynCamMaster，可以从不同视角同步生成视频内容，并保持多个视角下视频内容的一致性</title>
      <link>https://ansha886.github.io/posts/syncammaster/</link>
      <pubDate>Sat, 14 Dec 2024 11:40:31 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/syncammaster/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;商学合作，浙大联合快手等的多相机视频生成系统：SynCamMaster，可以从不同视角同步生成视频内容，并保持多个视角下视频内容的一致性&lt;/p&gt;</description>
    </item>
    <item>
      <title>一款本地自动化研究总结助手：research-rabbit，可以自动深入研究任何主题，提供带有源引用的完整研究报告</title>
      <link>https://ansha886.github.io/posts/de241dc/</link>
      <pubDate>Fri, 13 Dec 2024 09:20:41 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/de241dc/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <category domain="https://ansha886.github.io/categories/aigc/">AIGC</category>
      <description>&lt;p&gt;一款本地自动化研究/总结助手：research-rabbit，可以自动深入研究任何主题，提供带有源引用的完整研究报告&lt;/p&gt;</description>
    </item>
    <item>
      <title>Show Lab和微软开源的一个基于Qwen2VL架构开发的视觉-语言-动作多模态AI模型：ShowUI，它可以识别和理解用户界面元素，执行比如，点击、输入、选择、滚动等操作，实现GUI自动化</title>
      <link>https://ansha886.github.io/posts/show-lab-qwen2vl-ai/</link>
      <pubDate>Thu, 12 Dec 2024 09:16:07 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/show-lab-qwen2vl-ai/</guid>
      <category domain="https://ansha886.github.io/categories/aigc/">AIGC</category>
      <description>&lt;p&gt;Show Lab和微软开源的一个基于Qwen2VL架构开发的视觉-语言-动作多模态AI模型：ShowUI，它可以识别和理解用户界面元素，执行比如，点击、输入、选择、滚动等操作，实现GUI自动化。&lt;/p&gt;</description>
    </item>
    <item>
      <title>一个低成本高性能的AI修bug工具：Agentless，采用了一种无代理的方式，自动解决软件开发问题</title>
      <link>https://ansha886.github.io/posts/ai-bug-agentless/</link>
      <pubDate>Wed, 11 Dec 2024 21:58:48 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/ai-bug-agentless/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <category domain="https://ansha886.github.io/categories/agentless/">Agentless</category>
      <description>&lt;p&gt;Agentless是一款低成本高性能的AI修bug工具，采用无代理的方式，通过定位、修复和补丁验证的三步流程自动解决软件开发问题。在SWE-bench Lite上表现出色，成为所有开源方案中的最高性能工具。&lt;/p&gt;</description>
    </item>
    <item>
      <title>北大多模态Video-LLaVA模型：秒懂视频笑点的视觉语言大模型</title>
      <link>https://ansha886.github.io/posts/video-llava/</link>
      <pubDate>Mon, 09 Dec 2024 14:04:26 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/video-llava/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;Video-LLaVA模型的核心在于其能够提前将图片和视频的特征绑定到统一的特征空间中，这一策略极大地促进了模型对视觉信息的理解和处理能力。与传统的视觉语言模型相比，Video-LLaVA通过联合图片和视频的训练与指令微调，大幅提高了计算效率和模型性能。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Meta刚刚发布了其最新模型：Llama 3.3-70B，性能提升，输入成本比Llama 3.1 405B降低10倍！指令遵循能力超过了GPT-4o、Claude 3.5 Sonnet</title>
      <link>https://ansha886.github.io/posts/llama3point3/</link>
      <pubDate>Mon, 09 Dec 2024 11:20:53 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/llama3point3/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;Meta发布了最新的Llama 3.3-70B模型，具有70亿参数和128K上下文，输入成本比Llama 3.1 405B降低10倍，指令遵循能力超过GPT-4o和Claude 3.5。该模型支持英语、德语、法语等8种语言，适合多语言对话场景，具备良好的性价比，适用于构建聊天机器人等应用。&lt;/p&gt;</description>
    </item>
    <item>
      <title>音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度</title>
      <link>https://ansha886.github.io/posts/float/</link>
      <pubDate>Wed, 04 Dec 2024 10:43:04 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/float/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;&lt;strong&gt;这个效果非常可以，音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度&lt;/strong&gt;&#xA;一张源人物肖像图片+驱动音频，生成可以包含情感表现的面部动作并与音频同步的说话人视频它解决了时间连续的视频生成、由于迭代采样导致速度慢，这两个关键问题&lt;/p&gt;</description>
    </item>
    <item>
      <title>screen-to-code 一款使用 AI 将屏幕截图、模型和 Figma 设计转换为干净、实用的代码的简单工具。</title>
      <link>https://ansha886.github.io/posts/screenshot-to-code/</link>
      <pubDate>Tue, 03 Dec 2024 09:11:16 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/screenshot-to-code/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>screenshot-to-code 一款使用 AI 将屏幕截图、模型和 Figma 设计转换为干净、实用的代码的简单工具。现在支持 Claude Sonnet 3.5 和 GPT-4o！</description>
    </item>
    <item>
      <title>一个基于AI的多代理研究助手项目：AI-Data-Analysis-MultiAgent</title>
      <link>https://ansha886.github.io/posts/ai-data-analysis-multiagent-agent/</link>
      <pubDate>Tue, 26 Nov 2024 18:10:25 +0800</pubDate><author>licheng0601@gmail.com (Bruce)</author>
      <guid>https://ansha886.github.io/posts/ai-data-analysis-multiagent-agent/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>AI-Data-Analysis-MultiAgent是一个基于AI的多代理研究助手项目，能够自动化完成数据分析、生成研究假设、图表可视化和报告生成等任务。该项目利用LangChain、OpenAI和LangGraph构建，所有代理协作并具备逻辑思维，适用于报告、市场调研、数据分析和学术研究等领域。</description>
    </item>
  </channel>
</rss>
