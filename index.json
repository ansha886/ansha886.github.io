[{"content":"可以把各种文件转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置支持doc、docx、epub、html、htm、url、pdf、ppt、pptx、mp3、m4a等用Parser解析器从文件中提取文本和图像，用Converter转换器把提取的内容转为Markdown.\n可以把各种文件转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置\n支持doc、docx、epub、html、htm、url、pdf、ppt、pptx、mp3、m4a等\n用Parser解析器从文件中提取文本和图像，用Converter转换器把提取的内容转为Markdown\ngithub：https://github.com/wisupai/e2m\n","permalink":"https://ansha886.github.io/posts/markdown-e2m/","title":"可以把各种文件类型转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置"},{"content":"酷！DeepSeek刚刚开源了了DeepSeek V2.5的最终版微调模型： DeepSeek-V2.5-1210，新增联网搜索功能， 提升了数学、代码、写作、角色扮演等能力 优化了文件上传功能 新增联网搜索功能。\n酷！DeepSeek刚刚开源了了DeepSeek V2.5的最终版微调模型： DeepSeek-V2.5-1210，新增联网搜索功能\n提升了数学、代码、写作、角色扮演等能力 优化了文件上传功能 新增联网搜索功能\nDeepSeek-V2.5-1210联网搜索功能已上线网页端，登陆 https://chat.deepseek.com/，在输入框中打开“联网搜索”即可体验，目前API不支持搜索功能\n模型：https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210\n","permalink":"https://ansha886.github.io/posts/deepseek-v2-5-model/","title":"DeepSeek刚刚开源了了DeepSeek V2.5的最终版微调模型： DeepSeek-V2.5-1210，新增联网搜索功能"},{"content":"太牛了！Google最新的的量子计算芯片出来了：Willow！ Willow用时不到五分钟完成了一个标准基准计算，而当今最快的超级计算机需要10^25年，远远超过了宇宙的年龄\n太牛了！Google最新的的量子计算芯片出来了：Willow！\nWillow用时不到五分钟完成了一个标准基准计算，而当今最快的超级计算机需要10^25年，远远超过了宇宙的年龄\n博客：https://blog.google/technology/research/google-willow-quantum-chip/\n","permalink":"https://ansha886.github.io/posts/google-willow/","title":"这也太厉害了吧！Google最新的的量子计算芯片出来了：Willow！"},{"content":"Video-LLaVA模型的核心在于其能够提前将图片和视频的特征绑定到统一的特征空间中，这一策略极大地促进了模型对视觉信息的理解和处理能力。与传统的视觉语言模型相比，Video-LLaVA通过联合图片和视频的训练与指令微调，大幅提高了计算效率和模型性能。\n技术创新 Video-LLaVA引入了LanguageBind编码器，这一机制通过预先对齐图片和视频特征来形成统一的视觉表征。这种方法的优势在于无需预先训练各自的图片和视频编码器，从而简化了模型的训练过程，同时也降低了模型对数据的依赖。\n","permalink":"https://ansha886.github.io/posts/video-llava/","title":"北大多模态Video-LLaVA模型：秒懂视频笑点的视觉语言大模型"},{"content":"Meta发布了最新的Llama 3.3-70B模型，具有70亿参数和128K上下文，输入成本比Llama 3.1 405B降低10倍，指令遵循能力超过GPT-4o和Claude 3.5。该模型支持英语、德语、法语等8种语言，适合多语言对话场景，具备良好的性价比，适用于构建聊天机器人等应用。\nMeta刚刚发布了其最新模型：Llama 3.3-70B，性能提升，输入成本比Llama 3.1 405B降低10倍！指令遵循能力超过了GPT-4o、Claude 3.5 Sonnet\nLlama 3.3-70B是一个预训练和指令调优的多语言LLM，专门针对多语言对话场景进行了优化\n1、70B参数，128K上下文 2、多语言，支持英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语泰语8种语言\n它的通用能力强，多语言支持好，某些指标不如Claude 3.5等模型，数学和推理上有提升空间，总的来说是一个具备性价比的模型，可以构建聊天机器人等。\n型号卡：https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md\n模型：https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\n","permalink":"https://ansha886.github.io/posts/llama3point3/","title":"Meta刚刚发布了其最新模型：Llama 3.3-70B，性能提升，输入成本比Llama 3.1 405B降低10倍！指令遵循能力超过了GPT-4o、Claude 3.5 Sonnet"},{"content":"Cali是一个AI助手，可以用日常语言描述需求，自动构建运行iOS/Android的React Native应用，管理设备，处理npm包和CocoaPods依赖，并智能搜索React Native库。它可以独立使用，也可以与Vercel AI SDK、Claude、Zed等集成。\n一个可以写React Native应用的AI助手：Cali\n你可以用日常语言描述需求，它可以自动构建运行iOS/Android应用，管理手机/模拟器设备，自动处理npm包和CocoaPods依赖，智能搜索React Native库\n可独立使用，可以和Vercel AI SDK集成，也可以和Claude、Zed等配合使用\ngithub：https://github.com/callstackincubator/cali\n","permalink":"https://ansha886.github.io/posts/react-native-ai-cali/","title":"一个可以写React Native应用的AI助手：Cali"},{"content":"这个效果非常可以，音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度 一张源人物肖像图片+驱动音频，生成可以包含情感表现的面部动作并与音频同步的说话人视频它解决了时间连续的视频生成、由于迭代采样导致速度慢，这两个关键问题\n这个效果非常可以，音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度\n一张源人物肖像图片+驱动音频，生成可以包含情感表现的面部动作并与音频同步的说话人视频\n它解决了时间连续的视频生成、由于迭代采样导致速度慢，这两个关键问题\n项目：https://deepbrainai-research.github.io/float/\n![[dd9BxAP3tY98Zt8CPT12.48S.webp|dd9BxAP3tY98Zt8C - 00:12|50]] 00:12 ","permalink":"https://ansha886.github.io/posts/float/","title":"音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度"},{"content":"这也太厉害了吧！screenshot-to-code 是一款使用 AI 将屏幕截图、模型和 Figma 设计转换为干净、实用的代码的简单工具。现在支持 Claude Sonnet 3.5 和 GPT-4o！\n支持的堆栈：\nHTML + Tailwind HTML + CSS React + Tailwind Vue + Tailwind 引导 Ionic + Tailwind SVG 支持的 AI 模型：\nClaude Sonnet 3.5-最佳模型！ GPT-4o——也推荐！ DALL-E 3 或 Flux Schnell（使用 Replicate）用于图像生成 效果： ","permalink":"https://ansha886.github.io/posts/screenshot-to-code/","title":"screen-to-code 一款使用 AI 将屏幕截图、模型和 Figma 设计转换为干净、实用的代码的简单工具。"},{"content":"效果上，在保留了人物特征、前景/背景的条件下，同时保持了高质量的试穿效果 只需要参考服装图像+源姿势图像+源人物图像输入即可，相比现有方式更经济，用户友好。BooW-VTON主要通过结合数据增强技术和无掩码训练的新范式，从野外场景中生成大规模未配对的训练数据，提高了模型在这些复杂环境中的试穿性能。\n阿里等的虚拟试穿项目：BooW-VTON，它解决了在背景复杂多样的真实环境中进行高质量虚拟试穿的问题\n效果上，在保留了人物特征、前景/背景的条件下，同时保持了高质量的试穿效果 只需要参考服装图像+源姿势图像+源人物图像输入即可，相比现有方式更经济，用户友好\nBooW-VTON主要通过结合数据增强技术和无掩码训练的新范式，从野外场景中生成大规模未配对的训练数据，提高了模型在这些复杂环境中的试穿性能\n论文：https://arxiv.org/pdf/2408.06047 代码还没出呢，会在这里开源，github：https://github.com/little-misfit/BooW-VTON\n#虚拟试穿# #BooWVTON# #AI试衣#\n","permalink":"https://ansha886.github.io/posts/boow-vton/","title":"阿里等的虚拟试穿项目：BooW-VTON，它解决了在背景复杂多样的真实环境中进行高质量虚拟试穿的问题"},{"content":"酷！一个基于AI的多代理研究助手项目：AI-Data-Analysis-MultiAgent，这个agent团队可以自动化完成数据分析、生成研究假设、图表可视化、报告生成等整个流程任务所有agent互相配合，有专门的\u0026quot;笔记助手\u0026quot;记录所有进展，有质量审查，它们可以根据不同任务自动调整工作方式，具备逻辑思维。\n酷！一个基于AI的多代理研究助手项目：AI-Data-Analysis-MultiAgent，这个agent团队可以自动化完成数据分析、生成研究假设、图表可视化、报告生成等整个流程任务\n所有agent互相配合，有专门的\u0026quot;笔记助手\u0026quot;记录所有进展，有质量审查，它们可以根据不同任务自动调整工作方式，具备逻辑思维\n基于LangChain、OpenAI、LangGraph构建.\n可以用它做报告、市场调研、数据分析、学术研究等等.\n构建实时网络代理和浏览器自动化的一个开源工具：steel-browser，它提供了完整的REST API接口来控制浏览器操作\n可以基于它构建比如，AI网页助手、数据采集工具、自动化表格工具等，它可以执行打开网页、截图、下载文件等任务\n开箱即用，支持无头浏览器，支持Docker，支持反检测\n支持基本所有常见的网页操作\n支持并发处理，可以处理大规模任务\n自动处理异常和恢复\n","permalink":"https://ansha886.github.io/posts/ai-data-analysis-multiagent-agent/","title":"一个基于AI的多代理研究助手项目：AI-Data-Analysis-MultiAgent"}]