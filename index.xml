<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>XBruce Blog</title>
    <link>https://ansha886.github.io/</link>
    <description>@xbruce&#39;s works</description>
    <generator>Hugo 0.134.3 &amp; FixIt v0.3.15</generator>
    <language>en</language>
    <managingEditor>licheng0601@gmail.com (Bruce)</managingEditor>
    <webMaster>licheng0601@gmail.com (Bruce)</webMaster>
    <lastBuildDate>Mon, 23 Dec 2024 10:48:27 +0800</lastBuildDate>
    <atom:link href="https://ansha886.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>清华和腾讯也开源了一款图像自动上色AI模型：ColorFlow，可以给黑白图像序列上色，特点是能保持角色特征的一致性</title>
      <link>https://ansha886.github.io/posts/colorflow/</link>
      <pubDate>Mon, 23 Dec 2024 10:48:27 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/colorflow/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;清华和腾讯也开源了一款图像自动上色AI模型：ColorFlow，可以给黑白图像序列上色，特点是能保持角色特征的一致性&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;清华和腾讯也开源了一款图像自动上色AI模型：ColorFlow，可以给黑白图像序列上色，特点是能保持角色特征的一致性&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;用检索增强的方式上色&lt;/p&gt;&#xA;&lt;p&gt;人物发色、服装等元素的颜色可以保持很好的一致性&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/TencentARC/ColorFlow&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/TencentARC/ColorFlow&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ColorFlow.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ColorFlow.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ColorFlow.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/ColorFlow.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/ColorFlow.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ColorFlow.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Composio：一款面向AI智能体的生产级工具集</title>
      <link>https://ansha886.github.io/posts/composio/</link>
      <pubDate>Thu, 19 Dec 2024 11:08:45 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/composio/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;通过Composio，轻松为你的AI智能体集成高质量工具和服务，无需操心认证、准确性和可靠性——一行代码即可搞定！&lt;/p&gt;&#xA;&lt;p&gt;🔥 核心功能&lt;/p&gt;&#xA;&lt;p&gt;• 100+ 强大工具支持，涵盖多个领域：&lt;br&gt;&#xA;• 软件类：GitHub、Notion、Gmail、Slack、HubSpot、Salesforce等90+工具。&lt;br&gt;&#xA;• 操作系统类：模拟点击、输入文本、剪贴板操作等。&lt;br&gt;&#xA;• 浏览器类：智能搜索、截图、下载、上传等功能。&lt;br&gt;&#xA;• 搜索类：Google Search、Perplexity、Tavily、Exa等多种搜索引擎集成。&lt;br&gt;&#xA;• 开发工具：Ngrok、数据库、Redis、Vercel、Git等。&lt;br&gt;&#xA;• RAG功能：即时检索增强生成（RAG），支持任意类型数据处理。&lt;br&gt;&#xA;• 框架兼容性强：与主流AI智能体框架无缝集成：&lt;br&gt;&#xA;• OpenAI、Groq（兼容OpenAI API）、Claude、LlamaIndex、LangChain、CrewAI、Autogen、Gemini、Julep等。&lt;br&gt;&#xA;• 统一授权管理：内置六大认证协议，轻松实现授权：&lt;br&gt;&#xA;• 支持Access Token、Refresh Token、OAuth、API Key、JWT等，简化接入流程。&lt;br&gt;&#xA;• 高准确性：优化工具设计，提升40%+ 智能体工具调用的准确率。&lt;br&gt;&#xA;• 可嵌入 &amp;amp; 可扩展：&lt;br&gt;&#xA;• 可嵌入：支持后端集成，统一管理认证和工具，提供稳定一致的用户体验。&lt;br&gt;&#xA;• 可扩展：轻松添加新的工具、框架和认证协议，灵活满足业务需求。&lt;/p&gt;&#xA;&lt;p&gt;💡 技术亮点&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;一行代码集成：快速接入，简化开发流程。&lt;/li&gt;&#xA;&lt;li&gt;高度自定义：适配不同框架，按需组合工具。&lt;/li&gt;&#xA;&lt;li&gt;高性能与稳定性：生产级设计，确保工具调用的可靠性和响应速度。&lt;/li&gt;&#xA;&lt;li&gt;开发者友好：详细文档与示例，助力快速上手。&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Composio.gif&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Composio.gif&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Composio.gif?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/Composio.gif?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/Composio.gif?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Composio.gif&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>DiffHDR是一个用于修复历史文献的AI框架，能够预测并还原损坏文献的原始外观。</title>
      <link>https://ansha886.github.io/posts/diffhdr/</link>
      <pubDate>Thu, 19 Dec 2024 10:52:43 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/diffhdr/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;一个修复历史文献的AI框架：DiffHDR，它可以预测并修复损坏的历史文献的原始外观支持修复缺失字符、破损纸张、墨迹褪色可以准确还原原始文字内容和风格修复区域与周围背景协调一致&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;一个修复历史文献的AI框架：DiffHDR，它可以预测并修复损坏的历史文献的原始外观&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;支持修复缺失字符、破损纸张、墨迹褪色 可以准确还原原始文字内容和风格 修复区域与周围背景协调一致&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/yeungchenwa/HDR&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/yeungchenwa/HDR&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/logo.png&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/logo.png&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/logo.png?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/logo.png?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/logo.png?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/logo.png&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DiffHDR.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DiffHDR.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DiffHDR.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/DiffHDR.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/DiffHDR.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DiffHDR.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>VALL-E X：微软最新开源多语言文本到语音合成和语音克隆模型</title>
      <link>https://ansha886.github.io/posts/vall-e-x/</link>
      <pubDate>Mon, 16 Dec 2024 14:52:01 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/vall-e-x/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;VALL-E X 是 Microsoft 提出的一个令人惊叹的多语言文本转语音 (TTS) 模型，虽然 Microsoft 最初在其研究论文中发布了该模型，但他们没有发布任何代码或预训练模型，团队接受了重现结果的挑战。训练我们自己的模型。&lt;/p&gt;&#xA;&lt;p&gt;VALL-E X 是 Microsoft 提出的一个令人惊叹的多语言文本转语音 (TTS) 模型，虽然 Microsoft 最初在其研究论文中发布了该模型，但他们没有发布任何代码或预训练模型，团队接受了重现结果的挑战。训练我们自己的模型。我们很高兴与社区分享我们训练好的 VALL-E X 模型，让大家体验下一代 TTS 的强大功能 🎧&lt;/p&gt;&#xA;&lt;p&gt;github: &lt;a href=&#34;https://github.com/Plachtaa/VALL-E-X&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/Plachtaa/VALL-E-X&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/vallex_framework.jpg&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/vallex_framework.jpg&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/vallex_framework.jpg?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/vallex_framework.jpg?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/vallex_framework.jpg?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/vallex_framework.jpg&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>FreeStyle自由！西工大和微软等出的一个说唱(Rap)生成模型：Freestyler，可以根据歌词和伴奏直接生成说唱人声</title>
      <link>https://ansha886.github.io/posts/freestyler/</link>
      <pubDate>Mon, 16 Dec 2024 09:12:54 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/freestyler/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;FreeStyle自由！西工大和微软等出的一个说唱(Rap)生成模型：Freestyler，可以根据歌词和伴奏直接生成说唱人声&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;酷！西工大和微软等出的一个说唱(Rap)生成模型：Freestyler，可以根据歌词和伴奏直接生成说唱人声，风格以及节奏感跟伴奏匹配度还可以&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;歌词+伴奏音乐+3秒的参考人声，即可自动生成与伴奏节奏匹配的说唱人声&lt;/p&gt;&#xA;&lt;p&gt;可以模仿指定说唱歌手的音色，且能保持良好的节奏感和自然度&lt;/p&gt;&#xA;&lt;p&gt;目前开放了数据集 论文：&lt;a href=&#34;https://arxiv.org/abs/2408.15474&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/abs/2408.15474&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;数据集：HuggingFace: &lt;a href=&#34;https://huggingface.co/datasets/zqning/RapBank&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/datasets/zqning/RapBank&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;GitHub: &lt;a href=&#34;https://github.com/NZqian/RapBank&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/NZqian/RapBank&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Freestyler.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Freestyler.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Freestyler.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/Freestyler.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/Freestyler.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Freestyler.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>商学合作，浙大联合快手等的多相机视频生成系统：SynCamMaster，可以从不同视角同步生成视频内容，并保持多个视角下视频内容的一致性</title>
      <link>https://ansha886.github.io/posts/syncammaster/</link>
      <pubDate>Sat, 14 Dec 2024 11:40:31 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/syncammaster/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;商学合作，浙大联合快手等的多相机视频生成系统：SynCamMaster，可以从不同视角同步生成视频内容，并保持多个视角下视频内容的一致性&lt;/p&gt;&#xA;&lt;p&gt;浙大、快手等的多相机视频生成系统：SynCamMaster，可以从不同视角同步生成视频内容，并保持多个视角下视频内容的一致性。&lt;/p&gt;&#xA;&lt;p&gt;项目：&lt;a href=&#34;https://jianhongbai.github.io/SynCamMaster/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://jianhongbai.github.io/SynCamMaster/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/https-%3Ajianhongbai.github.io%3ASynCamMaster.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/https-%3Ajianhongbai.github.io%3ASynCamMaster.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/https-%3Ajianhongbai.github.io%3ASynCamMaster.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/https-%3Ajianhongbai.github.io%3ASynCamMaster.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/https-%3Ajianhongbai.github.io%3ASynCamMaster.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/https-%3Ajianhongbai.github.io%3ASynCamMaster.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>一款本地自动化研究总结助手：research-rabbit，可以自动深入研究任何主题，提供带有源引用的完整研究报告</title>
      <link>https://ansha886.github.io/posts/de241dc/</link>
      <pubDate>Fri, 13 Dec 2024 09:20:41 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/de241dc/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <category domain="https://ansha886.github.io/categories/aigc/">AIGC</category>
      <description>&lt;p&gt;一款本地自动化研究/总结助手：research-rabbit，可以自动深入研究任何主题，提供带有源引用的完整研究报告&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;一款本地自动化研究/总结助手：research-rabbit，可以自动深入研究任何主题，提供带有源引用的完整研究报告&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;根据主题，生成搜索词搜索网络信息，总结内容，发现不足，继续深入研究，最后生成完整研究报告&lt;/p&gt;&#xA;&lt;p&gt;支持免费网络搜索(Tavily API)，可设置研究迭代深度，通过LangGraph Studio可视化过程，输出markdown格式研究报告&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/langchain-ai/research-rabbit&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/langchain-ai/research-rabbit&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/research-rabbit.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/research-rabbit.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/research-rabbit.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/research-rabbit.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/research-rabbit.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/research-rabbit.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Show Lab和微软开源的一个基于Qwen2VL架构开发的视觉-语言-动作多模态AI模型：ShowUI，它可以识别和理解用户界面元素，执行比如，点击、输入、选择、滚动等操作，实现GUI自动化</title>
      <link>https://ansha886.github.io/posts/show-lab-qwen2vl-ai/</link>
      <pubDate>Thu, 12 Dec 2024 09:16:07 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/show-lab-qwen2vl-ai/</guid>
      <category domain="https://ansha886.github.io/categories/aigc/">AIGC</category>
      <description>&lt;p&gt;Show Lab和微软开源的一个基于Qwen2VL架构开发的视觉-语言-动作多模态AI模型：ShowUI，它可以识别和理解用户界面元素，执行比如，点击、输入、选择、滚动等操作，实现GUI自动化。&lt;/p&gt;&#xA;&lt;p&gt;能&amp;quot;看&amp;quot;屏幕、&amp;ldquo;懂&amp;quot;指令、会&amp;quot;操作&amp;rdquo;，可以帮你自动操作电脑或手机，不需要写代码，用自然语言即可&lt;/p&gt;&#xA;&lt;p&gt;不依赖源代码，它直接通过截图理解界面，自动识别和删减冗余信息，减少33%冗余视觉token，性能提升了1.4倍，零样本界面定位准确率为75.1%&lt;/p&gt;&#xA;&lt;p&gt;支持网页和手机界面&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/showlab/ShowUI&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/showlab/ShowUI&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>一个低成本高性能的AI修bug工具：Agentless，采用了一种无代理的方式，自动解决软件开发问题</title>
      <link>https://ansha886.github.io/posts/ai-bug-agentless/</link>
      <pubDate>Wed, 11 Dec 2024 21:58:48 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/ai-bug-agentless/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <category domain="https://ansha886.github.io/categories/agentless/">Agentless</category>
      <description>&lt;p&gt;Agentless是一款低成本高性能的AI修bug工具，采用无代理的方式，通过定位、修复和补丁验证的三步流程自动解决软件开发问题。在SWE-bench Lite上表现出色，成为所有开源方案中的最高性能工具。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;一个低成本高性能的AI修bug工具：Agentless，采用了一种无代理的方式，自动解决软件开发问题&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;在SWE-bench Lite上 在所有开源方案中实现了最高性能&lt;/p&gt;&#xA;&lt;p&gt;与Devin等复杂的自主代理方法不同，Agentless用三步解决问题：定位、修复、补丁验证，依据固定流程，LLM只负责在每个特定步骤中完成指定任务即可&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/OpenAutoCoder/Agentless&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/OpenAutoCoder/Agentless&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/2407.01489&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2407.01489&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>可以把各种文件类型转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置</title>
      <link>https://ansha886.github.io/posts/markdown-e2m/</link>
      <pubDate>Wed, 11 Dec 2024 15:48:49 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/markdown-e2m/</guid>
      <category domain="https://ansha886.github.io/categories/tools/">Tools</category>
      <description>&lt;p&gt;&lt;strong&gt;可以把各种文件转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置&lt;/strong&gt;支持doc、docx、epub、html、htm、url、pdf、ppt、pptx、mp3、m4a等用Parser解析器从文件中提取文本和图像，用Converter转换器把提取的内容转为Markdown.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;可以把各种文件转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;支持doc、docx、epub、html、htm、url、pdf、ppt、pptx、mp3、m4a等&lt;/p&gt;&#xA;&lt;p&gt;用Parser解析器从文件中提取文本和图像，用Converter转换器把提取的内容转为Markdown&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/wisupai/e2m&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/wisupai/e2m&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
