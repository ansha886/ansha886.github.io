<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>XBruce Blog</title>
    <link>https://ansha886.github.io/</link>
    <description>@xbruce&#39;s works</description>
    <generator>Hugo 0.134.3 &amp; FixIt v0.3.15</generator>
    <language>en</language>
    <managingEditor>licheng0601@gmail.com (Bruce)</managingEditor>
    <webMaster>licheng0601@gmail.com (Bruce)</webMaster>
    <lastBuildDate>Thu, 12 Dec 2024 09:16:07 +0800</lastBuildDate>
    <atom:link href="https://ansha886.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Show Lab和微软开源的一个基于Qwen2VL架构开发的视觉-语言-动作多模态AI模型：ShowUI，它可以识别和理解用户界面元素，执行比如，点击、输入、选择、滚动等操作，实现GUI自动化</title>
      <link>https://ansha886.github.io/posts/show-lab-qwen2vl-ai/</link>
      <pubDate>Thu, 12 Dec 2024 09:16:07 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/show-lab-qwen2vl-ai/</guid>
      <category domain="https://ansha886.github.io/categories/aigc/">AIGC</category>
      <description>&lt;p&gt;Show Lab和微软开源的一个基于Qwen2VL架构开发的视觉-语言-动作多模态AI模型：ShowUI，它可以识别和理解用户界面元素，执行比如，点击、输入、选择、滚动等操作，实现GUI自动化。&lt;/p&gt;&#xA;&lt;p&gt;能&amp;quot;看&amp;quot;屏幕、&amp;ldquo;懂&amp;quot;指令、会&amp;quot;操作&amp;rdquo;，可以帮你自动操作电脑或手机，不需要写代码，用自然语言即可&lt;/p&gt;&#xA;&lt;p&gt;不依赖源代码，它直接通过截图理解界面，自动识别和删减冗余信息，减少33%冗余视觉token，性能提升了1.4倍，零样本界面定位准确率为75.1%&lt;/p&gt;&#xA;&lt;p&gt;支持网页和手机界面&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/showlab/ShowUI&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/showlab/ShowUI&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/ShowUI.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>一个低成本高性能的AI修bug工具：Agentless，采用了一种无代理的方式，自动解决软件开发问题</title>
      <link>https://ansha886.github.io/posts/ai-bug-agentless/</link>
      <pubDate>Wed, 11 Dec 2024 21:58:48 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/ai-bug-agentless/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <category domain="https://ansha886.github.io/categories/agentless/">Agentless</category>
      <description>&lt;p&gt;Agentless是一款低成本高性能的AI修bug工具，采用无代理的方式，通过定位、修复和补丁验证的三步流程自动解决软件开发问题。在SWE-bench Lite上表现出色，成为所有开源方案中的最高性能工具。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;一个低成本高性能的AI修bug工具：Agentless，采用了一种无代理的方式，自动解决软件开发问题&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;在SWE-bench Lite上 在所有开源方案中实现了最高性能&lt;/p&gt;&#xA;&lt;p&gt;与Devin等复杂的自主代理方法不同，Agentless用三步解决问题：定位、修复、补丁验证，依据固定流程，LLM只负责在每个特定步骤中完成指定任务即可&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/OpenAutoCoder/Agentless&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/OpenAutoCoder/Agentless&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;论文：&lt;a href=&#34;https://arxiv.org/pdf/2407.01489&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://arxiv.org/pdf/2407.01489&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Agentless.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>可以把各种文件类型转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置</title>
      <link>https://ansha886.github.io/posts/markdown-e2m/</link>
      <pubDate>Wed, 11 Dec 2024 15:48:49 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/markdown-e2m/</guid>
      <category domain="https://ansha886.github.io/categories/tools/">Tools</category>
      <description>&lt;p&gt;&lt;strong&gt;可以把各种文件转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置&lt;/strong&gt;支持doc、docx、epub、html、htm、url、pdf、ppt、pptx、mp3、m4a等用Parser解析器从文件中提取文本和图像，用Converter转换器把提取的内容转为Markdown.&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;可以把各种文件转成Markdown的一个工具：E2M，每种格式有专门的解析器和转换器，支持自定义配置&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;支持doc、docx、epub、html、htm、url、pdf、ppt、pptx、mp3、m4a等&lt;/p&gt;&#xA;&lt;p&gt;用Parser解析器从文件中提取文本和图像，用Converter转换器把提取的内容转为Markdown&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/wisupai/e2m&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/wisupai/e2m&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/E2M.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepSeek刚刚开源了了DeepSeek V2.5的最终版微调模型： DeepSeek-V2.5-1210，新增联网搜索功能</title>
      <link>https://ansha886.github.io/posts/deepseek-v2-5-model/</link>
      <pubDate>Wed, 11 Dec 2024 15:39:11 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/deepseek-v2-5-model/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;&lt;strong&gt;酷！DeepSeek刚刚开源了了DeepSeek V2.5的最终版微调模型： DeepSeek-V2.5-1210，新增联网搜索功能&lt;/strong&gt;， 提升了数学、代码、写作、角色扮演等能力 优化了文件上传功能 新增联网搜索功能。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;酷！DeepSeek刚刚开源了了DeepSeek V2.5的最终版微调模型： DeepSeek-V2.5-1210，新增联网搜索功能&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;提升了数学、代码、写作、角色扮演等能力 优化了文件上传功能 新增联网搜索功能&lt;/p&gt;&#xA;&lt;p&gt;DeepSeek-V2.5-1210联网搜索功能已上线网页端，登陆 &lt;a href=&#34;https://chat.deepseek.com/%EF%BC%8C%E5%9C%A8%E8%BE%93%E5%85%A5%E6%A1%86%E4%B8%AD%E6%89%93%E5%BC%80%E2%80%9C%E8%81%94%E7%BD%91%E6%90%9C%E7%B4%A2%E2%80%9D%E5%8D%B3%E5%8F%AF%E4%BD%93%E9%AA%8C%EF%BC%8C%E7%9B%AE%E5%89%8DAPI%E4%B8%8D%E6%94%AF%E6%8C%81%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://chat.deepseek.com/%EF%BC%8C%E5%9C%A8%E8%BE%93%E5%85%A5%E6%A1%86%E4%B8%AD%E6%89%93%E5%BC%80%E2%80%9C%E8%81%94%E7%BD%91%E6%90%9C%E7%B4%A2%E2%80%9D%E5%8D%B3%E5%8F%AF%E4%BD%93%E9%AA%8C%EF%BC%8C%E7%9B%AE%E5%89%8DAPI%E4%B8%8D%E6%94%AF%E6%8C%81%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://chat.deepseek.com/，在输入框中打开“联网搜索”即可体验，目前API不支持搜索功能&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;模型：&lt;a href=&#34;https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/deepseek-ai/DeepSeek-V2.5-1210&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek1.gif&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek1.gif&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek1.gif?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek1.gif?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek1.gif?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/DeepSeek1.gif&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>这也太厉害了吧！Google最新的的量子计算芯片出来了：Willow！</title>
      <link>https://ansha886.github.io/posts/google-willow/</link>
      <pubDate>Tue, 10 Dec 2024 09:09:55 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/google-willow/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;&lt;strong&gt;太牛了！Google最新的的量子计算芯片出来了：Willow！&lt;/strong&gt;&#xA;Willow用时不到五分钟完成了一个标准基准计算，而当今最快的超级计算机需要10^25年，远远超过了宇宙的年龄&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;太牛了！Google最新的的量子计算芯片出来了：Willow！&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Willow用时不到五分钟完成了一个标准基准计算，而当今最快的超级计算机需要10^25年，远远超过了宇宙的年龄&lt;/p&gt;&#xA;&lt;p&gt;博客：&lt;a href=&#34;https://blog.google/technology/research/google-willow-quantum-chip/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://blog.google/technology/research/google-willow-quantum-chip/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Willow.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Willow.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Willow.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/Willow.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/Willow.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Willow.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>北大多模态Video-LLaVA模型：秒懂视频笑点的视觉语言大模型</title>
      <link>https://ansha886.github.io/posts/video-llava/</link>
      <pubDate>Mon, 09 Dec 2024 14:04:26 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/video-llava/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;Video-LLaVA模型的核心在于其能够提前将图片和视频的特征绑定到统一的特征空间中，这一策略极大地促进了模型对视觉信息的理解和处理能力。与传统的视觉语言模型相比，Video-LLaVA通过联合图片和视频的训练与指令微调，大幅提高了计算效率和模型性能。&lt;/p&gt;&#xA;&lt;h4 id=&#34;技术创新&#34; class=&#34;heading-element&#34;&gt;&lt;span&gt;技术创新&lt;/span&gt;&#xA;  &lt;a href=&#34;#%e6%8a%80%e6%9c%af%e5%88%9b%e6%96%b0&#34; class=&#34;heading-mark&#34;&gt;&#xA;    &lt;svg class=&#34;octicon octicon-link&#34; viewBox=&#34;0 0 16 16&#34; version=&#34;1.1&#34; width=&#34;16&#34; height=&#34;16&#34; aria-hidden=&#34;true&#34;&gt;&lt;path d=&#34;m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z&#34;&gt;&lt;/path&gt;&lt;/svg&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h4&gt;&lt;p&gt;Video-LLaVA引入了LanguageBind编码器，这一机制通过预先对齐图片和视频特征来形成统一的视觉表征。这种方法的优势在于无需预先训练各自的图片和视频编码器，从而简化了模型的训练过程，同时也降低了模型对数据的依赖。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/LLaVA-video.jpg&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/LLaVA-video.jpg&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/LLaVA-video.jpg?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/LLaVA-video.jpg?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/LLaVA-video.jpg?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/LLaVA-video.jpg&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Meta刚刚发布了其最新模型：Llama 3.3-70B，性能提升，输入成本比Llama 3.1 405B降低10倍！指令遵循能力超过了GPT-4o、Claude 3.5 Sonnet</title>
      <link>https://ansha886.github.io/posts/llama3point3/</link>
      <pubDate>Mon, 09 Dec 2024 11:20:53 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/llama3point3/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;Meta发布了最新的Llama 3.3-70B模型，具有70亿参数和128K上下文，输入成本比Llama 3.1 405B降低10倍，指令遵循能力超过GPT-4o和Claude 3.5。该模型支持英语、德语、法语等8种语言，适合多语言对话场景，具备良好的性价比，适用于构建聊天机器人等应用。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;Meta刚刚发布了其最新模型：Llama 3.3-70B，性能提升，输入成本比Llama 3.1 405B降低10倍！指令遵循能力超过了GPT-4o、Claude 3.5 Sonnet&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;Llama 3.3-70B是一个预训练和指令调优的多语言LLM，专门针对多语言对话场景进行了优化&lt;/p&gt;&#xA;&lt;p&gt;1、70B参数，128K上下文 2、多语言，支持英语、德语、法语、意大利语、葡萄牙语、印地语、西班牙语泰语8种语言&lt;/p&gt;&#xA;&lt;p&gt;它的通用能力强，多语言支持好，某些指标不如Claude 3.5等模型，数学和推理上有提升空间，总的来说是一个具备性价比的模型，可以构建聊天机器人等。&lt;/p&gt;&#xA;&lt;p&gt;型号卡：&lt;a href=&#34;https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;模型：&lt;a href=&#34;https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Llama%203.3&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Llama%203.3&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Llama%203.3?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/Llama%203.3?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/Llama%203.3?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Llama%203.3&#34; class=&#34;suffix-invalid suffix-invalid__small suffix-invalid__large&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>一个可以写React Native应用的AI助手：Cali</title>
      <link>https://ansha886.github.io/posts/react-native-ai-cali/</link>
      <pubDate>Fri, 06 Dec 2024 09:12:00 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/react-native-ai-cali/</guid>
      <category domain="https://ansha886.github.io/categories/ai/">AI</category>
      <description>&lt;p&gt;Cali是一个AI助手，可以用日常语言描述需求，自动构建运行iOS/Android的React Native应用，管理设备，处理npm包和CocoaPods依赖，并智能搜索React Native库。它可以独立使用，也可以与Vercel AI SDK、Claude、Zed等集成。&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;一个可以写React Native应用的AI助手：Cali&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;你可以用日常语言描述需求，它可以自动构建运行iOS/Android应用，管理手机/模拟器设备，自动处理npm包和CocoaPods依赖，智能搜索React Native库&lt;/p&gt;&#xA;&lt;p&gt;可独立使用，可以和Vercel AI SDK集成，也可以和Claude、Zed等配合使用&lt;/p&gt;&#xA;&lt;p&gt;github：&lt;a href=&#34;https://github.com/callstackincubator/cali&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://github.com/callstackincubator/cali&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Cali1.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Cali1.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Cali1.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/Cali1.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/Cali1.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/Cali1.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度</title>
      <link>https://ansha886.github.io/posts/float/</link>
      <pubDate>Wed, 04 Dec 2024 10:43:04 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/float/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;&lt;strong&gt;这个效果非常可以，音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度&lt;/strong&gt;&#xA;一张源人物肖像图片+驱动音频，生成可以包含情感表现的面部动作并与音频同步的说话人视频它解决了时间连续的视频生成、由于迭代采样导致速度慢，这两个关键问题&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;这个效果非常可以，音频驱动说话人肖像视频生成模型：FLOAT，保真度很高，且支持情感增强和控制，能调节情感表现的强度&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;一张源人物肖像图片+驱动音频，生成可以包含情感表现的面部动作并与音频同步的说话人视频&lt;/p&gt;&#xA;&lt;p&gt;它解决了时间连续的视频生成、由于迭代采样导致速度慢，这两个关键问题&lt;/p&gt;&#xA;&lt;p&gt;项目：&lt;a href=&#34;https://deepbrainai-research.github.io/float/&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;https://deepbrainai-research.github.io/float/&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/FLOAT1.webp&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/FLOAT1.webp&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/FLOAT1.webp?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/FLOAT1.webp?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/FLOAT1.webp?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/FLOAT1.webp&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;![[dd9BxAP3tY98Zt8CPT12.48S.webp|dd9BxAP3tY98Zt8C - 00:12|50]] &lt;a href=&#34;file:///Users/bruce/Downloads/dd9BxAP3tY98Zt8C.mp4#t=12.48&#34;target=&#34;_blank&#34; rel=&#34;external nofollow noopener noreferrer&#34;&gt;00:12&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;</description>
    </item>
    <item>
      <title>screen-to-code 一款使用 AI 将屏幕截图、模型和 Figma 设计转换为干净、实用的代码的简单工具。</title>
      <link>https://ansha886.github.io/posts/screenshot-to-code/</link>
      <pubDate>Tue, 03 Dec 2024 09:11:16 +0800</pubDate>
      <guid>https://ansha886.github.io/posts/screenshot-to-code/</guid>
      <category domain="https://ansha886.github.io/categories/llm/">LLM</category>
      <description>&lt;p&gt;这也太厉害了吧！screenshot-to-code 是一款使用 AI 将屏幕截图、模型和 Figma 设计转换为干净、实用的代码的简单工具。现在支持 Claude Sonnet 3.5 和 GPT-4o！&lt;/p&gt;&#xA;&lt;!--more--&gt;&#xA;&lt;p&gt;支持的堆栈：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;HTML + Tailwind&lt;/li&gt;&#xA;&lt;li&gt;HTML + CSS&lt;/li&gt;&#xA;&lt;li&gt;React + Tailwind&lt;/li&gt;&#xA;&lt;li&gt;Vue + Tailwind&lt;/li&gt;&#xA;&lt;li&gt;引导&lt;/li&gt;&#xA;&lt;li&gt;Ionic + Tailwind&lt;/li&gt;&#xA;&lt;li&gt;SVG&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;支持的 AI 模型：&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Claude Sonnet 3.5-最佳模型！&lt;/li&gt;&#xA;&lt;li&gt;GPT-4o——也推荐！&lt;/li&gt;&#xA;&lt;li&gt;DALL-E 3 或 Flux Schnell（使用 Replicate）用于图像生成&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;效果：&#xA;&lt;img loading=&#34;lazy&#34; src=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/screen-to-code.gif&#34; alt=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/screen-to-code.gif&#34; srcset=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/screen-to-code.gif?size=small, https://raw.githubusercontent.com/ansha886/blog-images/master/screen-to-code.gif?size=medium 1.5x, https://raw.githubusercontent.com/ansha886/blog-images/master/screen-to-code.gif?size=large 2x&#34; data-title=&#34;https://raw.githubusercontent.com/ansha886/blog-images/master/screen-to-code.gif&#34; style=&#34;background: url(/images/loading.min.svg) no-repeat center;&#34; onload=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}this.dataset.lazyloaded=&#39;&#39;;&#34; onerror=&#34;this.title=this.dataset.title;for(const i of [&#39;style&#39;, &#39;data-title&#39;,&#39;onerror&#39;,&#39;onload&#39;]){this.removeAttribute(i);}&#34;/&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
